{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['sepal_length', 'spal_width', 'petal_length', 'petal_width']\n",
    "features = pd.DataFrame(iris.data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = pd.DataFrame(iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = features.assign(type=target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, info_gain=None, value=None):\n",
    "        '''                 Constructor                 '''\n",
    "        # for decision node:\n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.info_gain = info_gain\n",
    "        \n",
    "        #for leaf:\n",
    "        # Majority Class of the leaf node\n",
    "        self.value = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier():\n",
    "    ''' Constructor '''\n",
    "    def __init__(self, min_sample_split=2, max_depth=2):\n",
    "        #Initalise the root of the tree\n",
    "        self.root = None\n",
    "        # Define Stopping Conditions:\n",
    "        self.max_depth = max_depth\n",
    "        self.min_sample_split = min_sample_split\n",
    "    ''' build tree method \n",
    "    \n",
    "    - Recursively build the tree by finding the best split until stopping \n",
    "    conditions are met \n",
    "    Steps: \n",
    "    - Stopping Condition:\n",
    "        if the number of samples during a split is less then the minimum \n",
    "        number of samples, or max depth of the tree is reached then return leaf \n",
    "        node (the prediction)\n",
    "    - Best Split: \n",
    "        Calls best split to find the best feature and threhold to \n",
    "        split on\n",
    "    - Create Leaf node: \n",
    "        if no valid split is found or stopping conditions are met, then return the leaf\n",
    "        node\n",
    "    '''\n",
    "    def build_tree(self, dataset, current_depth=0):\n",
    "        X = dataset.iloc[:,:-1]\n",
    "        Y = dataset.iloc[:,-1]\n",
    "        num_samples = np.shape(X)[0]\n",
    "        num_features = np.shape(X)[1]\n",
    "        \n",
    "       \n",
    "        \n",
    "        \n",
    "        if num_samples > self.min_sample_split & current_depth <= self.max_depth:\n",
    "            best_split = self.get_best_split(dataset, num_features)\n",
    "            if best_split[\"info_gain\"] > 0:\n",
    "                left_subtree = self.build_tree(best_split[\"dataset_left\"], current_depth + 1)\n",
    "                right_subtree = self.build_tree(best_split[\"dataset_right\"], current_depth + 1)\n",
    "                return Node(best_split[\"feature_index\"], best_split[\"threshold\"], left_subtree, right_subtree, best_split[\"info_gain\"])\n",
    "            \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ''' get_best_split \n",
    "    \n",
    "    For a given data set containing features and target values:\n",
    "    This function will for each feature will:\n",
    "    - define an empty dictionary to hold node values\n",
    "    - define max_information_gain as an infinitaly large number\n",
    "    \n",
    "        1. Get the feature values from dataset \n",
    "        2. get possible threhold values by finding the unique data points in each feature\n",
    "        for each threshold value in possible_threshold values:\n",
    "            1. get left and right datasets using the split function\n",
    "            2. if the legt and right datasets are not null:\n",
    "                3. list of classes in total (parent) list of classes in the left dataset (left_y) and list \n",
    "                of classes in the right data set (right_y)\n",
    "                4. Calcualte information gain as current_information_gain\n",
    "                5. if this is greater then the max_information gain:\n",
    "                    6. create best split dictionary where you have each nodes value: \n",
    "                    - feature_index\n",
    "                    - threshold\n",
    "                    - dataset_left\n",
    "                    - dataset_right\n",
    "                    - info_gain\n",
    "                7. Assign the max information gain as the current information gain\n",
    "                8. Return the best split dictionary\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    def get_best_split(self, dataset, num_features):\n",
    "        best_split = {}\n",
    "        max_info_gain = -float(\"inf\")\n",
    "        \n",
    "        for feature_index in range(num_features):\n",
    "            feature_values = dataset.iloc[:, feature_index]\n",
    "            possible_thresholds = np.unique(feature_values)\n",
    "            for threshold in possible_thresholds:\n",
    "                dataset_left, dataset_right = self.split(dataset, feature_index, threshold)\n",
    "                # check the datasets are not empty:\n",
    "                if len(dataset_left) > 0 & len(dataset_right)>0:\n",
    "                    y, left_y, right_y = dataset.iloc[:,-1], dataset_left[:,-1], dataset_right[:,-1]\n",
    "                    current_info_gain = self.info_gain(y, left_y, right_y, mode=\"entropy\")\n",
    "                    if current_info_gain > max_info_gain:\n",
    "                        best_split = {\n",
    "                            \"feature_index\" : feature_index,\n",
    "                            \"threshold\" : threshold,\n",
    "                            \"dataset_left\" : dataset_left,\n",
    "                            \"dataset_right\" : dataset_right,\n",
    "                            \"info_gain\" : current_info_gain\n",
    "                        }\n",
    "                        max_info_gain = current_info_gain\n",
    "        return best_split\n",
    "            \n",
    "        \n",
    "    \n",
    "    \n",
    "    ''' split \n",
    "    \n",
    "    This function given a threshold value will split the data set of each feature\n",
    "    and either assign it to the left subtree if the value is less then the threhold or \n",
    "    assign it to the right if the value is greater then the threshold\n",
    "\n",
    "    '''\n",
    "    def split(self, dataset, feature_index, threshold):\n",
    "        left_dataset = np.array(row for row in dataset[feature_index] <= threshold)\n",
    "        right_dataset = np.array(row for row in dataset[feature_index] > threshold)\n",
    "        return left_dataset, right_dataset\n",
    "    \n",
    "    \n",
    "    \n",
    "    ''' Information gain \n",
    "    1. Calculates the weight of the left and the right childs \n",
    "    if mode is entropy : \n",
    "    gain = entropy(parent) - (left_weight * entropy(left_child) + right_weight * entropy(right_child))\n",
    "    if mode is gini_index\n",
    "    gain = gini_index(parent) - (left_weight * gini_index(left_child) + right_weight * gini_index(right_child))\n",
    "    \n",
    "    '''\n",
    "    def info_gain(self, parent, left_child, right_child, mode=\"entropy\"):\n",
    "        weight_left = len(left_child) / len(parent)\n",
    "        weight_right = len(right_child) / len(parent)\n",
    "        if mode == \"gini\":\n",
    "            gain = self.gini_index(parent) - ((weight_left * self.gini_index(left_child)) + (weight_right * self.gini_index(right_child)))\n",
    "        else:\n",
    "            gain = self.entropy(parent) - ((weight_left * self.entropy(left_child)) + (weight_right * self.entropy(right_child)))\n",
    "        return gain\n",
    "    \n",
    "    ''' entropy \n",
    "    \n",
    "    returns the entropy of a given class \n",
    "    1. identify the unique classes you have, in the iris data set its 0, 1, 2 (three types)\n",
    "    2. For each unique class calculate entropy using the formula:\n",
    "    H(p1) = -p1 * Log2(p1)\n",
    "    Example: \n",
    "    y = np.array([0, 0, 1, 1, 1])\n",
    "    •\tStep 1: np.unique(y) returns the unique classes: [0, 1].\n",
    "\t•\tStep 2: We initialize entropy = 0.\n",
    "\t•\tStep 3: Loop through each class:\n",
    "\t•\tFor class 0:\n",
    "\t•\tProportion of class 0: p_0 = 2/5 = 0.4.\n",
    "\t•\tEntropy term: -0.4 * log2(0.4) ≈ 0.52877.\n",
    "\t•\tAdd this to entropy: entropy ≈ 0.52877.\n",
    "\t•\tFor class 1:\n",
    "\t•\tProportion of class 1: p_1 = 3/5 = 0.6.\n",
    "\t•\tEntropy term: -0.6 * log2(0.6) ≈ 0.44218.\n",
    "\t•\tAdd this to entropy: entropy ≈ 0.52877 + 0.44218 = 0.97095.\n",
    "    \n",
    "    '''\n",
    "    def entropy(slef, y):\n",
    "        # Find the unique classes in your target data set:\n",
    "        classes = np.unique(y)\n",
    "        # Initialise entropy to 0:\n",
    "        entropy = 0\n",
    "        for cls in classes:\n",
    "            # get the porpotion :\n",
    "            p_one = len(y[y == cls]) / len(y)\n",
    "            # calculate entropy for this porportion and add to total entropy for this class\n",
    "            entropy += -p_one * np.log2(p_one)\n",
    "        return entropy\n",
    "    \n",
    "    def gini_index(self, y):\n",
    "        class_labels = np.unique(y)\n",
    "        gini = 0\n",
    "        for cls in class_labels:\n",
    "            p_cls = len(y[y == cls]) / len(y)\n",
    "            gini += p_cls**2\n",
    "        return 1 - gini\n",
    "    \n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        ''' function to train the tree '''\n",
    "        dataset = np.concatenate((X, Y), axis=1)\n",
    "        self.root = self.build_tree(dataset)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        ''' function to predict new dataset '''\n",
    "        \n",
    "        preditions = [self.make_prediction(x, self.root) for x in X]\n",
    "        return preditions\n",
    "    \n",
    "    def make_prediction(self, x, tree):\n",
    "        ''' function to predict a single data point '''\n",
    "        \n",
    "        if tree.value!=None: return tree.value\n",
    "        feature_val = x[tree.feature_index]\n",
    "        if feature_val<=tree.threshold:\n",
    "            return self.make_prediction(x, tree.left)\n",
    "        else:\n",
    "            return self.make_prediction(x, tree.right)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
